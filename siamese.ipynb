{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFoVqrbQf9LSjXP6UEVVEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingrui-liu/Hyperpigmentation-autograde/blob/main/siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIKz83spVef2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import math\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential, Model, load_model,save_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation,GlobalAveragePooling2D,Lambda,Concatenate,Input,BatchNormalization\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "from tensorflow.keras.optimizers import SGD,Adam,RMSprop,Nadam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJxF1dwzVg7T"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "image_size = 512\n",
        "BATCH_SIZE = 32\n",
        "data_path = \"/content/drive/My Drive/Unilever/\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GtBnMNAVg90",
        "outputId": "ba53c432-772d-4272-9542-06f86c214dab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbvBjEcLVhAj"
      },
      "source": [
        "def create_generators():\n",
        "\n",
        "\tdef normalize(x):\n",
        "\t\tx /= 127.5\n",
        "\t\tx -= 1.\n",
        "\t\treturn x\n",
        "\n",
        "\tdef load_and_preprocess_image(path):\n",
        "\t\timage = tf.io.read_file(path)\n",
        "\t\timage = tf.image.decode_jpeg(image, channels=3)\n",
        "\t\timage = tf.image.resize(image, [image_size, image_size])\n",
        "\t\timage = tf.clip_by_value(image, 0.0, 255.0)\n",
        "\t\timage = tf.cast(image, dtype=tf.uint8)\n",
        "\n",
        "\t\treturn image\n",
        "\n",
        "\tdef load_file_from_disk(data_path):\n",
        "\t\tall_image_paths = sorted(glob.glob(data_path + \"image_test/*.jpg\"))\n",
        "\n",
        "\t\t# read the excel file\n",
        "\t\tMHPgrades = pd.read_excel(data_path + \"MHPgrades.xlsx\")\n",
        "\t\tgrade = MHPgrades[\"Mottled hyperpigmentation\"]\n",
        "\n",
        "\t\t#group and generate the lable\n",
        "\t\tall_image_labels = list(grade)\n",
        "\t\tall_label = []\n",
        "\t\tfor grade in all_image_labels:\n",
        "\t\t\tif grade < 3.0:\n",
        "\t\t\t\tlabel = 2.5\n",
        "\t\t\telif grade > 5.0:\n",
        "\t\t\t\tlabel = 5.5\n",
        "\t\t\telse:\n",
        "\t\t\t\tlabel = grade\n",
        "\t\t\tall_label.append(int(label * 2))\n",
        "\n",
        "\t\tlabel_to_index = dict((name, index) for index, name in enumerate(sorted(set(all_label))))\n",
        "\t\tall_labels = [label_to_index[grade] for grade in all_label]\n",
        "\t\t              \n",
        "\t\t# train test split \n",
        "\t\ttrain_paths, test_paths, train_labels, test_labels = train_test_split(all_image_paths, all_labels,test_size = 0.1)\n",
        "\n",
        "\t\treturn train_paths, test_paths, train_labels, test_labels\n",
        "\n",
        "\n",
        "\tdef load_file_from_disk_with_additional_image(data_pat):\n",
        "\t\tall_image_paths_1 = sorted(glob.glob(data_path + \"image_test/*.jpg\"))\n",
        "\t\tall_image_paths_2 = sorted(glob.glob(data_path + \"Additional_Images/*.jpg\"))\n",
        "\n",
        "\t\t# read the excel file\n",
        "\t\tMHPgrades = pd.read_excel(data_path +\"MHPgrades.xlsx\",header = 0)\n",
        "\t\tMHPgrades2 = pd.read_excel(data_path +\"SKN-APP-0452-MHPgrades.xlsx\",header = 1)\n",
        "\n",
        "\t\tMHPgrades2 = MHPgrades2.dropna(axis = 0)\n",
        "\t\tMHPgrades2 = MHPgrades2.melt(id_vars=[\"Subject No.\", \"Side of Face\"], \n",
        "\t\t        var_name=\"VISIT\", \n",
        "\t\t        value_name=\"Mottled hyperpigmentation\").sort_values(by = ['Subject No.','Side of Face'])\n",
        "\n",
        "\t\tgrade1 = MHPgrades[\"Mottled hyperpigmentation\"]\n",
        "\n",
        "\t\t#Manipulate excel file\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Baseline' , 'VISIT'] = '00'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 4' , 'VISIT'] = '04'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 8' , 'VISIT'] = '08'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 12' , 'VISIT'] = '12'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 16' , 'VISIT'] = '16'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 17' , 'VISIT'] = '17'\n",
        "\n",
        "\t\tMHPgrades2.loc[MHPgrades2['Side of Face'] == 'Left' , 'Side of Face'] = 'L'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['Side of Face'] == 'Right' , 'Side of Face'] = 'R'\n",
        "\n",
        "\t\t#match grade and images and make valid pair\n",
        "\t\tvalid_path = []\n",
        "\t\tvalid_label = []\n",
        "\t\tfor index, row in MHPgrades2.iterrows():\n",
        "\t\t    id = str(row['Subject No.'] )\n",
        "\t\t    id = id.zfill(4)\n",
        "\n",
        "\t\t    side = row['Side of Face']\n",
        "\t\t   \n",
        "\t\t    visit = str(row['VISIT'])\n",
        "\n",
        "\t\t    grade = row['Mottled hyperpigmentation']\n",
        "\t\t  \n",
        "\n",
        "\n",
        "\t\t    output = [path for path in all_image_paths_2 if path.startswith('/content/drive/My Drive/Unilever/Additional_Images/P' + id + '_FACE_' + side + '_S00_VCR_S2_V00D00W' + visit)] \n",
        "\t\t    if len(output)>0:\n",
        "\t\t      valid_path.append(output[0])\n",
        "\t\t      valid_label.append(grade)\n",
        "\n",
        "\t\tall_image_paths = all_image_paths_1 + valid_path\n",
        "\t\tall_image_labels = list(grade1) +  valid_label\n",
        "\n",
        "\t\t##Create label and zip dataset\n",
        "\t\tall_label = []\n",
        "\t\tfor grade in all_image_labels:\n",
        "\t\t  if grade < 3.0:\n",
        "\t\t    label = 2.5\n",
        "\t\t  elif grade > 5.0:\n",
        "\t\t    label = 5.5\n",
        "\t\t  else:\n",
        "\t\t    label = grade\n",
        "\t\t  all_label.append(label)\n",
        "\n",
        "\t\tlabel_to_index = dict((name, index) for index, name in enumerate(sorted(set(all_label))))\n",
        "\t\tall_labels = [label_to_index[grade] for grade in all_label]\n",
        "\n",
        "\t\t##train val test split\n",
        "\t\ttrain_paths, test_paths, train_labels, test_labels = train_test_split(all_image_paths, all_labels, test_size=0.1, random_state=1)\n",
        "\n",
        "\t\treturn train_paths, test_paths, train_labels, test_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdef create_ds(paths,labels):\n",
        "\t\ttrain_paths_1 = tf.data.Dataset.from_tensor_slices(paths)\n",
        "\t\ttrain_paths_2 = tf.data.Dataset.from_tensor_slices(paths)\n",
        "\t\timage_ds_1 = train_paths_1.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "\t\timage_ds_2 = train_paths_2.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "\t\tlabel_ds_1 = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.float32))\n",
        "\t\tlabel_ds_2 = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "\t\timage_label_ds_1 = tf.data.Dataset.zip((image_ds_1, label_ds_1))\n",
        "\t\timage_label_ds_1 = image_label_ds_1.shuffle(64)\n",
        "\n",
        "\t\timage_label_ds_2 = tf.data.Dataset.zip((image_ds_2, label_ds_2))\n",
        "\t\timage_label_ds_2 = image_label_ds_2.shuffle(64)\n",
        "\t\t\n",
        "\n",
        "\t\timage_label_ds = tf.data.Dataset.zip((image_label_ds_1, image_label_ds_2))\n",
        "\n",
        "\t\treturn image_label_ds\n",
        "\n",
        "\tdef detect(label1,label2):\n",
        "\n",
        "\t\tlabel = tf.cond(tf.equal(label1,label2), lambda: tf.constant(1.0), lambda: tf.constant(0.0))\n",
        "\n",
        "\t\treturn label\n",
        "\n",
        "\n",
        "\n",
        "\ttrain_paths, test_paths, train_labels, test_labels = load_file_from_disk_with_additional_image(data_path)\n",
        "\t# here's our final training dataset\n",
        "\ttrain_ds = create_ds(train_paths,train_labels)\n",
        "\ttest_ds = create_ds(test_paths,test_labels)\n",
        "\n",
        "\ttrain_ds = train_ds.cache()\n",
        "\t\n",
        "\ttrain_ds = train_ds.shuffle(buffer_size = 64)\n",
        "\ttrain_ds = train_ds.repeat()\n",
        "\ttrain_ds = train_ds.map(lambda image_label1,image_label2: (tf.image.random_flip_left_right(image_label1[0]),tf.image.random_flip_left_right(image_label2[0]),image_label1[1],image_label2[1]),num_parallel_calls=AUTOTUNE)\n",
        "\t#train_ds = train_ds.map(lambda image1,image2,label1,label2:(randaugment.distort_image_with_randaugment(image1,num_layers = 1,magnitude = 2),\n",
        "\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandaugment.distort_image_with_randaugment(image2,num_layers = 1,magnitude = 2),\n",
        "\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlabel1,\n",
        "\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlabel2),num_parallel_calls=AUTOTUNE)\n",
        "\ttrain_ds = train_ds.map(lambda image1,image2,label1,label2 :(normalize(tf.cast(image1,tf.float32)),normalize(tf.cast(image2,tf.float32)),label1,label2),num_parallel_calls=AUTOTUNE)\n",
        "\ttrain_ds = train_ds.map(lambda image1,image2,label1,label2 :((image1,image2),detect(label1,label2)),num_parallel_calls=AUTOTUNE)\n",
        "\ttrain_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "\ttest_ds = test_ds.cache()\n",
        "\ttest_ds = test_ds.shuffle(buffer_size = 64)\n",
        "\ttest_ds = test_ds.repeat()\n",
        "\ttest_ds = test_ds.map(lambda image_label1,image_label2: (image_label1[0],image_label2[0],image_label1[1],image_label2[1]),num_parallel_calls=AUTOTUNE)\n",
        "\ttest_ds = test_ds.map(lambda image1,image2,label1,label2 :(normalize(tf.cast(image1,tf.float32)),normalize(tf.cast(image2,tf.float32)),label1,label2),num_parallel_calls=AUTOTUNE)\n",
        "\ttest_ds = test_ds.map(lambda image1,image2,label1,label2 :((image1,image2),detect(label1,label2)),num_parallel_calls=AUTOTUNE)\n",
        "\ttest_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "\treturn train_ds,test_ds\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCjTNukkVxEq",
        "outputId": "0b8cc277-904b-4add-84f2-5167de1eec8f"
      },
      "source": [
        "def create_base_model():\n",
        "    conv_base = ResNet50(include_top = False, weights = 'imagenet',\n",
        "                          input_shape = (224, 224, 3))\n",
        "\n",
        "\n",
        "    #conv_base.trainable = False\n",
        "    x = conv_base.output\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    embedding = GlobalAveragePooling2D()(x)\n",
        "    embedding = Dense(128)(embedding)    \n",
        "    return Model(conv_base.input, embedding)\n",
        "\n",
        "def SiameseNetwork(base_model):\n",
        "    \"\"\"\n",
        "    Create the siamese model structure using the supplied base and head model.\n",
        "    \"\"\"\n",
        "    input_a = Input(shape=(224, 224, 3),name = \"image1\")\n",
        "    input_b = Input(shape=(224, 224, 3),name = \"image2\")\n",
        "\n",
        "    processed_a = base_model(input_a)\n",
        "    processed_b = base_model(input_b)\n",
        "\n",
        "\n",
        "\n",
        "    head = Concatenate()([processed_a, processed_b])\n",
        "    head = Dense(1)(head)\n",
        "    head = Activation(activation='sigmoid')(head)\n",
        "    return Model([input_a, input_b], head)\n",
        "\n",
        "base_model = create_base_model()\n",
        "\n",
        "\n",
        "\n",
        "siamese_network = SiameseNetwork(base_model)\n",
        "\n",
        "siamese_network.save(\"test.h5\")\n",
        "lr_schedule  = tfa.optimizers.ExponentialCyclicalLearningRate(\n",
        "                              initial_learning_rate=1e-8,\n",
        "                              maximal_learning_rate=1e-6,\n",
        "                              step_size=240,\n",
        "                              )\n",
        "opt = Adam(learning_rate=1e-8)\n",
        "\n",
        "siamese_network.compile(optimizer= opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['RootMeanSquaredError'])\n",
        "\n",
        "train_ds,test_ds = create_generators()\n",
        "\n",
        "\n",
        "\n",
        "history = siamese_network.fit(train_ds,\n",
        "      epochs = 100,\n",
        "      steps_per_epoch = 100,\n",
        "      validation_data = test_ds,\n",
        "      validation_steps = 20)\n",
        "      \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"image1_3:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 512, 512, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"image2_3:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 512, 512, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_4:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 512, 512, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_4:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 512, 512, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"image1_3:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 512, 512, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"image2_3:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 512, 512, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_4:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 512, 512, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_4:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 512, 512, 3).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rlLipDPVy6Q"
      },
      "source": [
        "def plot_acc(history):\n",
        "  \n",
        "    plt.title('Training and validation root_mean_squared_error')\n",
        "    plt.plot(history.history['accuracy'], label='accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.plot(history.history['loss'][7:], label='loss')\n",
        "    plt.plot(history.history['val_loss'][7:], label = 'val_loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def plot_rmse(history):\n",
        "  \n",
        "    plt.title('Training and validation RMSE')\n",
        "    plt.plot(history.history['RootMeanSquaredError'], label='RMSE')\n",
        "    plt.plot(history.history['val_RootMeanSquaredError'], label = 'val_RMSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.plot(history.history['loss'][7:], label='loss')\n",
        "    plt.plot(history.history['val_loss'][7:], label = 'val_loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_rmse(history)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}