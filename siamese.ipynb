{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM03TiYgkfYyPtOf7IPV/Ve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingrui-liu/Hyperpigmentation-autograde/blob/main/siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIKz83spVef2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import math\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential, Model, load_model,save_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation,GlobalAveragePooling2D,Lambda,Concatenate,Input,BatchNormalization\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "from tensorflow.keras.optimizers import SGD,Adam,RMSprop,Nadam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJxF1dwzVg7T"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "image_size = 224\n",
        "BATCH_SIZE = 32\n",
        "data_path = \"/content/drive/My Drive/Unilever/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GtBnMNAVg90",
        "outputId": "da61e97c-5c00-4a38-f8c0-5a7527aebcbf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbvBjEcLVhAj"
      },
      "source": [
        "def create_generators():\n",
        "\n",
        "\tdef normalize(x):\n",
        "\t\tx /= 127.5\n",
        "\t\tx -= 1.\n",
        "\t\treturn x\n",
        "\n",
        "\tdef load_and_preprocess_image(path):\n",
        "\t\timage = tf.io.read_file(path)\n",
        "\t\timage = tf.image.decode_jpeg(image, channels=3)\n",
        "\t\timage = tf.image.resize(image, [image_size, image_size])\n",
        "\t\timage = tf.clip_by_value(image, 0.0, 255.0)\n",
        "\t\timage = tf.cast(image, dtype=tf.uint8)\n",
        "\n",
        "\t\treturn image\n",
        "\n",
        "\tdef load_file_from_disk(data_path):\n",
        "\t\tall_image_paths = sorted(glob.glob(data_path + \"image_test/*.jpg\"))\n",
        "\n",
        "\t\t# read the excel file\n",
        "\t\tMHPgrades = pd.read_excel(data_path + \"MHPgrades.xlsx\")\n",
        "\t\tgrade = MHPgrades[\"Mottled hyperpigmentation\"]\n",
        "\n",
        "\t\t#group and generate the lable\n",
        "\t\tall_image_labels = list(grade)\n",
        "\t\tall_label = []\n",
        "\t\tfor grade in all_image_labels:\n",
        "\t\t\tif grade < 3.0:\n",
        "\t\t\t\tlabel = 2.5\n",
        "\t\t\telif grade > 5.0:\n",
        "\t\t\t\tlabel = 5.5\n",
        "\t\t\telse:\n",
        "\t\t\t\tlabel = grade\n",
        "\t\t\tall_label.append(int(label * 2))\n",
        "\n",
        "\t\tlabel_to_index = dict((name, index) for index, name in enumerate(sorted(set(all_label))))\n",
        "\t\tall_labels = [label_to_index[grade] for grade in all_label]\n",
        "\t\t              \n",
        "\t\t# train test split \n",
        "\t\ttrain_paths, test_paths, train_labels, test_labels = train_test_split(all_image_paths, all_labels,test_size = 0.1)\n",
        "\n",
        "\t\treturn train_paths, test_paths, train_labels, test_labels\n",
        "\n",
        "\n",
        "\tdef load_file_from_disk_with_additional_image(data_pat):\n",
        "\t\tall_image_paths_1 = sorted(glob.glob(data_path + \"image_test/*.jpg\"))\n",
        "\t\tall_image_paths_2 = sorted(glob.glob(data_path + \"Additional_Images/*.jpg\"))\n",
        "\n",
        "\t\t# read the excel file\n",
        "\t\tMHPgrades = pd.read_excel(data_path +\"MHPgrades.xlsx\",header = 0)\n",
        "\t\tMHPgrades2 = pd.read_excel(data_path +\"SKN-APP-0452-MHPgrades.xlsx\",header = 1)\n",
        "\n",
        "\t\tMHPgrades2 = MHPgrades2.dropna(axis = 0)\n",
        "\t\tMHPgrades2 = MHPgrades2.melt(id_vars=[\"Subject No.\", \"Side of Face\"], \n",
        "\t\t        var_name=\"VISIT\", \n",
        "\t\t        value_name=\"Mottled hyperpigmentation\").sort_values(by = ['Subject No.','Side of Face'])\n",
        "\n",
        "\t\tgrade1 = MHPgrades[\"Mottled hyperpigmentation\"]\n",
        "\n",
        "\t\t#Manipulate excel file\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Baseline' , 'VISIT'] = '00'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 4' , 'VISIT'] = '04'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 8' , 'VISIT'] = '08'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 12' , 'VISIT'] = '12'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 16' , 'VISIT'] = '16'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['VISIT'] == 'Week 17' , 'VISIT'] = '17'\n",
        "\n",
        "\t\tMHPgrades2.loc[MHPgrades2['Side of Face'] == 'Left' , 'Side of Face'] = 'L'\n",
        "\t\tMHPgrades2.loc[MHPgrades2['Side of Face'] == 'Right' , 'Side of Face'] = 'R'\n",
        "\n",
        "\t\t#match grade and images and make valid pair\n",
        "\t\tvalid_path = []\n",
        "\t\tvalid_label = []\n",
        "\t\tfor index, row in MHPgrades2.iterrows():\n",
        "\t\t    id = str(row['Subject No.'] )\n",
        "\t\t    id = id.zfill(4)\n",
        "\n",
        "\t\t    side = row['Side of Face']\n",
        "\t\t   \n",
        "\t\t    visit = str(row['VISIT'])\n",
        "\n",
        "\t\t    grade = row['Mottled hyperpigmentation']\n",
        "\t\t  \n",
        "\n",
        "\n",
        "\t\t    output = [path for path in all_image_paths_2 if path.startswith('/content/drive/My Drive/Unilever/Additional_Images/P' + id + '_FACE_' + side + '_S00_VCR_S2_V00D00W' + visit)] \n",
        "\t\t    if len(output)>0:\n",
        "\t\t      valid_path.append(output[0])\n",
        "\t\t      valid_label.append(grade)\n",
        "\n",
        "\t\tall_image_paths = all_image_paths_1 + valid_path\n",
        "\t\tall_image_labels = list(grade1) +  valid_label\n",
        "\n",
        "\t\t##Create label and zip dataset\n",
        "\t\tall_label = []\n",
        "\t\tfor grade in all_image_labels:\n",
        "\t\t  if grade < 3.0:\n",
        "\t\t    label = 2.5\n",
        "\t\t  elif grade > 5.0:\n",
        "\t\t    label = 5.5\n",
        "\t\t  else:\n",
        "\t\t    label = grade\n",
        "\t\t  all_label.append(label)\n",
        "\n",
        "\t\tlabel_to_index = dict((name, index) for index, name in enumerate(sorted(set(all_label))))\n",
        "\t\tall_labels = [label_to_index[grade] for grade in all_label]\n",
        "\n",
        "\t\ttrain_paths, test_paths, train_labels, test_labels = train_test_split(all_image_paths, all_labels, test_size=0.1, random_state=1)\n",
        "\t\ttrain_paths, val_paths, train_labels, val_labels = train_test_split(train_paths, train_labels, test_size=0.1, random_state=1)\n",
        "\n",
        "\t\treturn train_paths,val_paths, test_paths, train_labels, val_labels, test_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdef create_ds(paths,labels):\n",
        "\t\ttrain_paths_1 = tf.data.Dataset.from_tensor_slices(paths)\n",
        "\t\ttrain_paths_2 = tf.data.Dataset.from_tensor_slices(paths)\n",
        "\t\timage_ds_1 = train_paths_1.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "\t\timage_ds_2 = train_paths_2.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "\t\tlabel_ds_1 = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.float32))\n",
        "\t\tlabel_ds_2 = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "\t\timage_label_ds_1 = tf.data.Dataset.zip((image_ds_1, label_ds_1))\n",
        "\t\timage_label_ds_1 = image_label_ds_1.shuffle(64)\n",
        "\n",
        "\t\timage_label_ds_2 = tf.data.Dataset.zip((image_ds_2, label_ds_2))\n",
        "\t\timage_label_ds_2 = image_label_ds_2.shuffle(64)\n",
        "\t\t\n",
        "\n",
        "\t\timage_label_ds = tf.data.Dataset.zip((image_label_ds_1, image_label_ds_2))\n",
        "\n",
        "\t\treturn image_label_ds\n",
        "\n",
        "\tdef detect(label1,label2):\n",
        "\n",
        "\t\tlabel = tf.cond(tf.equal(label1,label2), lambda: tf.constant(1.0), lambda: tf.constant(0.0))\n",
        "\n",
        "\t\treturn label\n",
        "\n",
        "\n",
        "\n",
        "\ttrain_paths,val_paths, test_paths, train_labels, val_labels, test_labels = load_file_from_disk_with_additional_image(data_path)\n",
        "  # here's our final training dataset\n",
        "\ttrain_ds = create_ds(train_paths,train_labels)\n",
        "\tval_ds = create_ds(val_paths,val_labels)\n",
        "\ttest_ds = create_ds(test_paths,test_labels)\n",
        "\n",
        "\n",
        "  #train\n",
        "\ttrain_ds = train_ds.cache()\n",
        "\ttrain_ds = train_ds.shuffle(buffer_size = 1024)\n",
        "\ttrain_ds = train_ds.repeat()\n",
        "\ttrain_ds = train_ds.map(lambda image_label1,image_label2: (tf.image.random_flip_left_right(image_label1[0]),tf.image.random_flip_left_right(image_label2[0]),image_label1[1],image_label2[1]),num_parallel_calls=AUTOTUNE)\n",
        "\t#train_ds = train_ds.map(lambda image1,image2,label1,label2:(randaugment.distort_image_with_randaugment(image1,num_layers = 1,magnitude = 2),\n",
        "\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandaugment.distort_image_with_randaugment(image2,num_layers = 1,magnitude = 2),\n",
        "\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlabel1,\n",
        "\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlabel2),num_parallel_calls=AUTOTUNE)\n",
        "\ttrain_ds = train_ds.map(lambda image1,image2,label1,label2 :(normalize(tf.cast(image1,tf.float32)),normalize(tf.cast(image2,tf.float32)),label1,label2),num_parallel_calls=AUTOTUNE)\n",
        "\ttrain_ds = train_ds.map(lambda image1,image2,label1,label2 :((image1,image2),detect(label1,label2)),num_parallel_calls=AUTOTUNE)\n",
        "\ttrain_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  #val\n",
        "\tval_ds = val_ds.cache()\n",
        "\tval_ds = val_ds.shuffle(buffer_size = 64)\n",
        "\tval_ds = val_ds.repeat()\n",
        "\tval_ds = val_ds.map(lambda image_label1,image_label2: (image_label1[0],image_label2[0],image_label1[1],image_label2[1]),num_parallel_calls=AUTOTUNE)\n",
        "\tval_ds = val_ds.map(lambda image1,image2,label1,label2 :(normalize(tf.cast(image1,tf.float32)),normalize(tf.cast(image2,tf.float32)),label1,label2),num_parallel_calls=AUTOTUNE)\n",
        "\tval_ds = val_ds.map(lambda image1,image2,label1,label2 :((image1,image2),detect(label1,label2)),num_parallel_calls=AUTOTUNE)\n",
        "\tval_ds = val_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  #test\n",
        "\ttest_ds = test_ds.cache()\n",
        "\ttest_ds = test_ds.shuffle(buffer_size = 64)\n",
        "\ttest_ds = test_ds.repeat()\n",
        "\ttest_ds = test_ds.map(lambda image_label1,image_label2: (image_label1[0],image_label2[0],image_label1[1],image_label2[1]),num_parallel_calls=AUTOTUNE)\n",
        "\ttest_ds = test_ds.map(lambda image1,image2,label1,label2 :(normalize(tf.cast(image1,tf.float32)),normalize(tf.cast(image2,tf.float32)),label1,label2),num_parallel_calls=AUTOTUNE)\n",
        "\ttest_ds = test_ds.map(lambda image1,image2,label1,label2 :((image1,image2),detect(label1,label2)),num_parallel_calls=AUTOTUNE)\n",
        "\ttest_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\treturn train_ds,val_ds, test_ds\n",
        "\n",
        "train_ds,val_ds, test_ds = create_generators()\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCjTNukkVxEq",
        "outputId": "3a304a26-0531-41e3-e674-9c6d485e7917"
      },
      "source": [
        "def create_base_model():\n",
        "    conv_base = ResNet50(include_top = False, weights = 'imagenet',\n",
        "                          input_shape = (224, 224, 3))\n",
        "\n",
        "\n",
        "    #conv_base.trainable = False\n",
        "    x = conv_base.output\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    embedding = GlobalAveragePooling2D()(x)\n",
        "    embedding = Dense(128)(embedding)    \n",
        "    return Model(conv_base.input, embedding)\n",
        "\n",
        "def SiameseNetwork(base_model):\n",
        "    \"\"\"\n",
        "    Create the siamese model structure using the supplied base and head model.\n",
        "    \"\"\"\n",
        "    input_a = Input(shape=(224, 224, 3),name = \"image1\")\n",
        "    input_b = Input(shape=(224, 224, 3),name = \"image2\")\n",
        "\n",
        "    processed_a = base_model(input_a)\n",
        "    processed_b = base_model(input_b)\n",
        "\n",
        "\n",
        "\n",
        "    head = Concatenate()([processed_a, processed_b])\n",
        "    head = Dense(1)(head)\n",
        "    head = Activation(activation='sigmoid')(head)\n",
        "    return Model([input_a, input_b], head)\n",
        "\n",
        "base_model = create_base_model()\n",
        "\n",
        "\n",
        "\n",
        "siamese_network = SiameseNetwork(base_model)\n",
        "\n",
        "#siamese_network.save(\"test.h5\")\n",
        "lr_schedule  = tfa.optimizers.ExponentialCyclicalLearningRate(\n",
        "                              initial_learning_rate=1e-8,\n",
        "                              maximal_learning_rate=1e-6,\n",
        "                              step_size=240,\n",
        "                              )\n",
        "opt = Adam(learning_rate=1e-8)\n",
        "\n",
        "siamese_network.compile(optimizer= opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy','RootMeanSquaredError'])\n",
        "\n",
        "\n",
        "\n",
        "history = siamese_network.fit(train_ds,\n",
        "      epochs = 100,\n",
        "      steps_per_epoch = 50,\n",
        "      validation_data = val_ds,\n",
        "      validation_steps = 20)\n",
        "      \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "50/50 [==============================] - 38s 757ms/step - loss: 0.5409 - accuracy: 0.7706 - root_mean_squared_error: 0.4222 - val_loss: 2.8894 - val_accuracy: 0.1344 - val_root_mean_squared_error: 0.8966\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 35s 698ms/step - loss: 0.5422 - accuracy: 0.7656 - root_mean_squared_error: 0.4223 - val_loss: 2.9792 - val_accuracy: 0.1344 - val_root_mean_squared_error: 0.8990\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 36s 711ms/step - loss: 0.5548 - accuracy: 0.7538 - root_mean_squared_error: 0.4286 - val_loss: 3.2086 - val_accuracy: 0.1359 - val_root_mean_squared_error: 0.9040\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 35s 707ms/step - loss: 0.5286 - accuracy: 0.7750 - root_mean_squared_error: 0.4158 - val_loss: 3.4651 - val_accuracy: 0.1328 - val_root_mean_squared_error: 0.9095\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 35s 706ms/step - loss: 0.5500 - accuracy: 0.7500 - root_mean_squared_error: 0.4263 - val_loss: 3.2807 - val_accuracy: 0.1344 - val_root_mean_squared_error: 0.9001\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 35s 708ms/step - loss: 0.5208 - accuracy: 0.7719 - root_mean_squared_error: 0.4115 - val_loss: 2.8364 - val_accuracy: 0.1312 - val_root_mean_squared_error: 0.8855\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 35s 708ms/step - loss: 0.5384 - accuracy: 0.7631 - root_mean_squared_error: 0.4202 - val_loss: 2.0800 - val_accuracy: 0.1453 - val_root_mean_squared_error: 0.8245\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 35s 708ms/step - loss: 0.5135 - accuracy: 0.7912 - root_mean_squared_error: 0.4088 - val_loss: 1.0884 - val_accuracy: 0.2672 - val_root_mean_squared_error: 0.6437\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 35s 707ms/step - loss: 0.5286 - accuracy: 0.7756 - root_mean_squared_error: 0.4159 - val_loss: 0.6879 - val_accuracy: 0.5672 - val_root_mean_squared_error: 0.4960\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 35s 706ms/step - loss: 0.5284 - accuracy: 0.7781 - root_mean_squared_error: 0.4146 - val_loss: 0.5932 - val_accuracy: 0.6953 - val_root_mean_squared_error: 0.4511\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 35s 707ms/step - loss: 0.5230 - accuracy: 0.7825 - root_mean_squared_error: 0.4124 - val_loss: 0.5480 - val_accuracy: 0.7359 - val_root_mean_squared_error: 0.4275\n",
            "Epoch 12/100\n",
            "21/50 [===========>..................] - ETA: 17s - loss: 0.5618 - accuracy: 0.7574 - root_mean_squared_error: 0.4306"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "2rlLipDPVy6Q",
        "outputId": "02a38293-03aa-4e2a-db33-edc2a80a3d71"
      },
      "source": [
        "def plot_acc(history):\n",
        "  \n",
        "    plt.title('Training and validation root_mean_squared_error')\n",
        "    plt.plot(history.history['accuracy'], label='accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "def plo_loss(history):\n",
        "\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.plot(history.history['loss'], label='loss')\n",
        "    plt.plot(history.history['val_loss'], label = 'val_loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_rmse(history):\n",
        "  \n",
        "    plt.title('Training and validation RMSE')\n",
        "    plt.plot(history.history['root_mean_squared_error'], label='RMSE')\n",
        "    plt.plot(history.history['val_root_mean_squared_error'], label = 'val_RMSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(history)\n",
        "plot_rmse(history)\n",
        "plot_acc(history)\n",
        "    \n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i2nDc0WOwBU"
      },
      "source": [
        "prediction = model.predict_classes(test_ds)\n",
        "\n",
        "classes=[0,1,2,3,4,5,6]\n",
        "con_mat  = tf.math.confusion_matrix(\n",
        "    test_labels, prediction, num_classes=None, weights=None, dtype=tf.dtypes.int32,\n",
        "    name=None\n",
        ").numpy()\n",
        "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                     index = classes, \n",
        "                     columns = classes)\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWHeC1kyENvP",
        "outputId": "2786902a-70db-49bd-dd8c-6db9b37bfe3a"
      },
      "source": [
        "loss, accuracy, RMSE = siamese_network.evaluate(test_ds,steps= 32)\n",
        "print(\"Loss: {:1.2}\".format(loss))\n",
        "print(\"RMSE: {:1.2}\".format (RMSE))\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 9s 279ms/step - loss: 0.6442 - accuracy: 0.6406 - root_mean_squared_error: 0.4755\n",
            "RMSE: 0.4754846692085266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y--vQot9Ng_D"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "siamese_network.save('/content/drive/MyDrive/Capstone/progress2/project/siamese_network.h5')  # creates a HDF5 file 'my_model.h5'"
      ],
      "execution_count": 55,
      "outputs": []
    }
  ]
}